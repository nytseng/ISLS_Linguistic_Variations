<div align="center">
<h2>Evaluating Multilingual Large Language Models Using Linguistic Variations In Multilingual Learnersâ€™ Writing: A Teacher Study</h2>

ðŸš© Accepted by ISLS 2025!

[Paper (available on ResearchGate)](https://www.researchgate.net/publication/392690110_Evaluating_Language_Models_Using_Linguistic_Variations_in_Multilingual_Learners'_Writing_A_Teacher_Study)

[Kaycie Barron](https://scholar.google.com/citations?hl=zh-CN&tzom=-480&user=ZCDjTn8AAAAJ)<sup>1</sup>
| [Nora Tseng](https://scholar.google.com/citations?user=A-U8zE8AAAAJ&hl=zh-CN)<sup>1</sup> | 
[Shamya Karumbaiah](https://shamya.github.io/)<sup>1</sup> | 
[Cynthia Baeza](https://www.linkedin.com/in/cynthia-baeza/)<sup>1</sup> | 

<sup>1</sup>University of Wisconsin-Madison
</div>

This paper investigates teachersâ€™ perceptions on linguistic variations in bi/multilingual learnersâ€™ (MLs) writing to evaluate the (in)effectiveness of Multilingual Large Language Models (MLLMs), which are artificial intelligence (AI) models that generate texts in multiple languages. Due to their inherent linguistic biases, these models often struggle to interpret MLsâ€™ linguistic variations. To address this gap, we elicit teacher feedback on prevalent linguistic variations in MLsâ€™ writing and assess how Meta Llama 3.1, a state-of-the-art MLLM, responds to these variations. Using translanguaging as a lensâ€”the fluid use of multiple languages to convey meaning across social contextsâ€”we propose a new approach to evaluate MLLMs in multilingual learning contexts. With the increasing prevalence of AI in K12 classrooms, this paper advocates for the inclusion of bi/multilingual educators to better align the use of AI with progressive pedagogies such as translanguaging.

## 6 Linguistic Variations that we identified
<img width="885" alt="Screenshot 2024-11-15 at 3 29 05â€¯PM" src="https://github.com/user-attachments/assets/a547bf94-092d-4f1d-b0ea-a2d95ccf27b5">

Synthetically generated variations (n=200) available in eval_datasets.zip

## Other sources
[Original datasets](https://github.com/asr9koa/Code-switching-lak) from previous work

